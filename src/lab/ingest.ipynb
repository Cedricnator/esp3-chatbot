{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa375956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\areva\\Desktop\\U\\ESPE3\\esp3-chatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a710bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pages_text(pdf_path):\n",
    "    \"\"\"Return list of page texts (1-indexed pages -> index 0 = page 1)\"\"\"\n",
    "    # pdfminer.extract_text supports page_numbers argument\n",
    "    from pdfminer.pdfpage import PDFPage\n",
    "    pages = []\n",
    "    # Get total pages by iterating once\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        for _ in PDFPage.get_pages(f):\n",
    "            pages.append(\"\")  # initialize with empty string (never None)\n",
    "    total = len(pages)\n",
    "    for i in range(total):\n",
    "        try:\n",
    "            text = extract_text(pdf_path, page_numbers=[i])\n",
    "            pages[i] = text or \"\"  # ensure we store a string, not None\n",
    "        except Exception:\n",
    "            # If extraction fails for a page (e.g. images-only), keep empty string\n",
    "            pages[i] = \"\"\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd5ae956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_repeated_header_footer(page_texts, head_lines=3, tail_lines=3, sample_pages=10):\n",
    "    \"\"\"Heuristic: look for lines that repeat across many pages in the first/last N lines.\n",
    "    Returns (header_pattern, footer_pattern) regex strings (may be None).\n",
    "    \"\"\"\n",
    "    sample = page_texts\n",
    "    if len(page_texts) > sample_pages:\n",
    "        # sample evenly\n",
    "        idxs = np.linspace(0, len(page_texts) - 1, sample_pages, dtype=int)\n",
    "        sample = [page_texts[i] for i in idxs]\n",
    "\n",
    "    headers = []\n",
    "    footers = []\n",
    "    for p in sample:\n",
    "        if not p:\n",
    "            continue  # skip empty / None pages\n",
    "        lines = [l.strip() for l in p.splitlines() if l.strip()]\n",
    "        if not lines:\n",
    "            continue\n",
    "        headers.append(\"\\n\".join(lines[:head_lines]))\n",
    "        footers.append(\"\\n\".join(lines[-tail_lines:]))\n",
    "\n",
    "    def common_pattern(strings, threshold_ratio=0.4):\n",
    "        if not strings:\n",
    "            return None\n",
    "        cnt = Counter(strings)\n",
    "        common, freq = cnt.most_common(1)[0]\n",
    "        if freq / len(strings) >= threshold_ratio:\n",
    "            # escape regex special chars, but allow digits (page numbers) variability\n",
    "            # replace runs of digits with \\d+\n",
    "            esc = re.escape(common)\n",
    "            esc = re.sub(r'\\\\\\d\\+', r'\\\\d\\+', esc)  # no-op if none\n",
    "            esc = re.sub(r'\\\\\\d{1,}', r'\\\\d+', esc)\n",
    "            # also collapse variable whitespace\n",
    "            esc = re.sub(r'\\\\\\s\\+', r'\\\\s+', esc)\n",
    "            return esc\n",
    "        return None\n",
    "\n",
    "    header_pat = common_pattern(headers)\n",
    "    footer_pat = common_pattern(footers)\n",
    "    return header_pat, footer_pat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32d1b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_page_text(text, header_pat=None, footer_pat=None):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    s = text\n",
    "    # remove header\n",
    "    if header_pat:\n",
    "        try:\n",
    "            s = re.sub(r'(?m)^' + header_pat + r'\\s*\\n?', '', s)\n",
    "        except re.error:\n",
    "            pass\n",
    "    if footer_pat:\n",
    "        try:\n",
    "            s = re.sub(r'(?m)\\n?\\s*' + footer_pat + r'$','', s)\n",
    "        except re.error:\n",
    "            pass\n",
    "    # remove multiple blank lines, normalize spaces\n",
    "    s = re.sub(r'\\r', '\\n', s)\n",
    "    s = re.sub(r'\\n{3,}', '\\n\\n', s)\n",
    "    s = re.sub(r'[ \\t]{2,}', ' ', s)\n",
    "    # trim lines\n",
    "    s = '\\n'.join([ln.strip() for ln in s.splitlines() if ln.strip()])\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee332e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_sentence_split(text):\n",
    "    # naive sentence splitter\n",
    "    if not text:\n",
    "        return []\n",
    "    # protect abbreviations (very naive)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z0-9])', text)\n",
    "    # fallback to line-based\n",
    "    if len(sentences) == 1:\n",
    "        sentences = [ln for ln in text.splitlines() if ln.strip()]\n",
    "    return [s.strip() for s in sentences if s.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5473647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_chars=1000, overlap=200):\n",
    "    sentences = simple_sentence_split(text)\n",
    "    chunks = []\n",
    "    cur = \"\"\n",
    "    cur_len = 0\n",
    "    start_idx = 0\n",
    "    for sent in sentences:\n",
    "        if cur_len + len(sent) + 1 <= max_chars:\n",
    "            if cur:\n",
    "                cur += ' ' + sent\n",
    "            else:\n",
    "                cur = sent\n",
    "            cur_len = len(cur)\n",
    "        else:\n",
    "            chunks.append((cur, start_idx, start_idx + cur_len))\n",
    "            # start new chunk with overlap\n",
    "            # compute overlap in characters from end of cur\n",
    "            overlap_text = cur[-overlap:] if overlap and overlap < len(cur) else cur\n",
    "            cur = overlap_text + ' ' + sent\n",
    "            start_idx = start_idx + cur_len - len(overlap_text)\n",
    "            cur_len = len(cur)\n",
    "    if cur:\n",
    "        chunks.append((cur, start_idx, start_idx + cur_len))\n",
    "    # give chunk ids and order\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4cb4d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row, max_chars=1000, overlap=200):\n",
    "    pdf_path = Path(row['path'])\n",
    "    pages = extract_pages_text(str(pdf_path))\n",
    "    header_pat, footer_pat = detect_repeated_header_footer(pages)\n",
    "\n",
    "    rows = []\n",
    "    doc_id = str(uuid.uuid4())\n",
    "    title_guess = row['nombre'] or pdf_path.stem\n",
    "    chunk_counter = 0\n",
    "\n",
    "    for i, p in enumerate(pages, start=1):\n",
    "        cleaned = clean_page_text(p, header_pat, footer_pat)\n",
    "        if i == 1 and not row['nombre']:\n",
    "            first_line = cleaned.splitlines()[0] if cleaned.splitlines() else ''\n",
    "            if len(first_line) > 10:\n",
    "                title_guess = first_line[:200]\n",
    "\n",
    "        chunks = chunk_text(cleaned, max_chars=max_chars, overlap=overlap)\n",
    "        for text, start_char, end_char in chunks:\n",
    "            chunk_id = f\"{doc_id}_p{i}_c{chunk_counter}\"\n",
    "            rows.append({\n",
    "                'chunk_id': chunk_id,\n",
    "                'doc_id': doc_id,\n",
    "                'title': title_guess,\n",
    "                'page': i,\n",
    "                'text': text,\n",
    "                'start_char': int(start_char),\n",
    "                'end_char': int(end_char),\n",
    "                'url': row['url'],\n",
    "                'fecha': row['fecha'],\n",
    "                'vigencia': row['vigencia'],\n",
    "            })\n",
    "            chunk_counter += 1\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcfaf91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(csv_path, output_dir, model_name='all-MiniLM-L6-v2',\n",
    "                max_chars=1000, overlap=200, index_type='ip'):\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df_meta = pd.read_csv(csv_path)\n",
    "    all_rows = []\n",
    "    for _, row in df_meta.iterrows():\n",
    "        rows = process_row(row, max_chars=max_chars, overlap=overlap)\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    if df.empty:\n",
    "        raise RuntimeError('No text extracted from PDFs')\n",
    "    \n",
    "    \n",
    "    model = SentenceTransformer(model_name)\n",
    "    texts = df['text'].tolist()\n",
    "    embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "    dim = embeddings.shape[1]\n",
    "    if index_type.lower() in ('ip', 'indexflatip'):\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        faiss.normalize_L2(embeddings)\n",
    "    else:\n",
    "        index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    index_path = output_dir / 'index.faiss'\n",
    "    faiss.write_index(index, str(index_path))\n",
    "\n",
    "    chunks_path = output_dir / 'chunks.parquet'\n",
    "    df.to_parquet(chunks_path, index=False)\n",
    "\n",
    "    mapping = pd.DataFrame({\n",
    "        'chunk_id': df['chunk_id'].tolist(),\n",
    "        'position': list(range(len(df)))\n",
    "    })\n",
    "    mapping.to_parquet(output_dir / 'mapping.parquet', index=False)\n",
    "\n",
    "    print(f\"Saved FAISS index to: {index_path}\")\n",
    "    print(f\"Saved chunks to: {chunks_path}\")\n",
    "    print(f\"Saved mapping to: {output_dir / 'mapping.parquet'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7734ec1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\areva\\Desktop\\U\\ESPE3\\esp3-chatbot\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\areva\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Batches: 100%|██████████| 10/10 [00:07<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved FAISS index to: ..\\..\\data\\processed\\index.faiss\n",
      "Saved chunks to: ..\\..\\data\\processed\\chunks.parquet\n",
      "Saved mapping to: ..\\..\\data\\processed\\mapping.parquet\n",
      "Done: None\n"
     ]
    }
   ],
   "source": [
    "csv_path = '../../data/sources.csv'\n",
    "output_dir = '../../data/processed/'\n",
    "\n",
    "\n",
    "res = process_csv(csv_path, output_dir)\n",
    "print('Done:', res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esp3-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
